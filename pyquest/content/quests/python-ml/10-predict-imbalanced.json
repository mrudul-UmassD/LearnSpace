{
  "id": "10-predict-imbalanced",
  "world": "python-ml",
  "title": "Predict: Accuracy vs F1 on Imbalanced Data",
  "story": "Test your intuition! On highly imbalanced data with a naive baseline, which metric will be higher?",
  "instructions": "Predict the output. Data: 90% negative, 10% positive. Model predicts ALL negative. Will accuracy or F1 be higher? Print the name of higher metric.",
  "type": "predict_output",
  "starterCode": "import numpy as np\n\ny_true = np.array([0]*90 + [1]*10)\ny_pred = np.array([0]*100)\n\n# Calculate metrics\naccuracy = (y_true == y_pred).sum() / len(y_true)\n\nTP = ((y_true == 1) & (y_pred == 1)).sum()\nFP = ((y_true == 0) & (y_pred == 1)).sum()\nFN = ((y_true == 1) & (y_pred == 0)).sum()\n\nif TP + FP == 0:\n    precision = 0\nelse:\n    precision = TP / (TP + FP)\n    \nif TP + FN == 0:\n    recall = 0\nelse:\n    recall = TP / (TP + FN)\n\nif precision + recall == 0:\n    f1 = 0\nelse:\n    f1 = 2 * (precision * recall) / (precision + recall)\n\nif accuracy > f1:\n    print('accuracy')\nelse:\n    print('f1')",
  "solutionHidden": "accuracy",
  "tests": [
    {
      "id": "test1",
      "type": "output",
      "description": "Should print 'accuracy' (0.9 vs F1=0)",
      "expectedBehavior": "Accuracy misleadingly high on imbalanced data",
      "expected": "accuracy"
    }
  ],
  "hints": [
    {
      "level": 1,
      "text": "All negative predictions: accuracy=90%, recall=0, so F1=0"
    },
    {
      "level": 2,
      "text": "Accuracy=0.9 (90/100 correct), F1=0 (no positives caught). Accuracy higher but misleading!"
    }
  ],
  "hintUnlockAttempts": 2,
  "xpReward": 30,
  "difficulty": "intermediate",
  "order": 10
}
